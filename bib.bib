@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{olah2018building,
  title={The building blocks of interpretability},
  author={Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
  journal={Distill},
  volume={3},
  number={3},
  pages={e10},
  year={2018}
}

@article{conmy2024towards,
  title={Towards automated circuit discovery for mechanistic interpretability},
  author={Conmy, Arthur and Mavor-Parker, Augustine and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adri{\`a}},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{conmy2024towards,
  title={Towards automated circuit discovery for mechanistic interpretability},
  author={Conmy, Arthur and Mavor-Parker, Augustine and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adri{\`a}},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{wang2022interpretability,
  title={Interpretability in the wild: a circuit for indirect object identification in gpt-2 small},
  author={Wang, Kevin and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2211.00593},
  year={2022}
}

@article{lindner2024tracr,
  title={Tracr: Compiled transformers as a laboratory for interpretability},
  author={Lindner, David and Kram{\'a}r, J{\'a}nos and Farquhar, Sebastian and Rahtz, Matthew and McGrath, Tom and Mikulik, Vladimir},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{weiss2021thinking,
  title={Thinking like transformers},
  author={Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  booktitle={International Conference on Machine Learning},
  pages={11080--11090},
  year={2021},
  organization={PMLR}
}

@article{yang2019evaluating,
  title={Evaluating explanation without ground truth in interpretable machine learning},
  author={Yang, Fan and Du, Mengnan and Hu, Xia},
  journal={arXiv preprint arXiv:1907.06831},
  year={2019}
}

@article{friedman2024learning,
  title={Learning transformer programs},
  author={Friedman, Dan and Wettig, Alexander and Chen, Danqi},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{tamkin2023codebook,
  title={Codebook features: Sparse and discrete interpretability for neural networks},
  author={Tamkin, Alex and Taufeeque, Mohammad and Goodman, Noah D},
  journal={arXiv preprint arXiv:2310.17230},
  year={2023}
}

@article{michaud2024opening,
  title={Opening the AI black box: program synthesis via mechanistic interpretability},
  author={Michaud, Eric J and Liao, Isaac and Lad, Vedang and Liu, Ziming and Mudide, Anish and Loughridge, Chloe and Guo, Zifan Carl and Kheirkhah, Tara Rezaei and Vukeli{\'c}, Mateja and Tegmark, Max},
  journal={arXiv preprint arXiv:2402.05110},
  year={2024}
}

@article{krueger2017bayesian,
  title={Bayesian hypernetworks},
  author={Krueger, David and Huang, Chin-Wei and Islam, Riashat and Turner, Ryan and Lacoste, Alexandre and Courville, Aaron},
  journal={arXiv preprint arXiv:1710.04759},
  year={2017}
}

@article{liao2023generating,
  title={Generating interpretable networks using hypernetworks},
  author={Liao, Isaac and Liu, Ziming and Tegmark, Max},
  journal={arXiv preprint arXiv:2312.03051},
  year={2023}
}

@article{elhage2022toy,
  title={Toy models of superposition},
  author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and others},
  journal={arXiv preprint arXiv:2209.10652},
  year={2022}
}

@article{sutton2019bitter,
  title={The bitter lesson},
  author={Sutton, Richard},
  journal={Incomplete Ideas (blog)},
  volume={13},
  number={1},
  year={2019}
}

@article{elhage2021mathematical,
  title={A mathematical framework for transformer circuits},
  author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and others},
  journal={Transformer Circuits Thread},
  volume={1},
  year={2021}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{heimersheim2023circuit,
  title={A circuit for Python docstrings in a 4-layer attention-only transformer},
  author={Heimersheim, Stefan and Janiak, Jett},
  journal={URL: https://www. alignmentforum. org/posts/u6KXXmKFbXfWzoAXn/acircuit-for-python-docstrings-in-a-4-layer-attention-only},
  year={2023}
}